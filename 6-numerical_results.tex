\chapter{Electromagnetic Worldlines - Numerical Methods and Results}
\label{ch:numerical}
\begin{enumerate}
\item Need to develop methods that can easily generalize to more complicated geometries.
\item Even if using them to solve known problems.
\item In same fashion path integral relies on chaining together correct short-time propagator, we 
  are effectively using planar results at each step, under the assumption that the geometry
    is well-described locally by a single nearby plane.  
\item Need to study convergence effects carefully.  
\end{enumerate}


\section{Monte-Carlo sampling}

\begin{enumerate}
\item Need average over ensemble of paths.
\item For a Dirichlet scalar, can determine the set of times when integral is non-zero and 
  integrate over that.  In this case, the loop potential is either zero or one, with no variation.
  Thus it is more straightforward to integrate over space/loop-time.  
\item For computationally expensive loops, it makes sense to sample only a single position, time.
  Explore a larger ensemble in the same computational time.
  \item Rely on importance sampling to determine most important regions of $x_0$, $T$ to sample from.
  \item For time: For TE loops, integrand is zero before first touching time.  
    So use $P(\cT,\cT_0) = 2\cT_0^2\theta(\cT-\cT_0)/\cT^3$.
    TM loops turn on more softly, due to potential terms (which take into account possible sub-paths
    that touch the surface).
  \item Birth-death method effectively examines a larger ensemble of loops.
    For two-bodies, it is possible to spawn new loops while close to only a single surface,
    that might touch the other surface.  Thus the first-touching time for the parent loop
    is a poor estimate.  Simplest to use an ensemble average estimate for time:
    $P(\cT,d_0) = \exp(-2d_0^2/\cT)/\cT^3$, where exponential factor is the first touching
    time.  
  \item For spatial integrals, sample from uniform distribution within a radius that bounds all objects.
    Then sample from a power-law outside that, where estimate follows from first-touching time.
    May be possible to optimize, but captures important features without biasing
  \item Frequency integrals: Can execute in parallel (like picking different $chi$).
\end{enumerate}

\section{Sampling Times}

For a given discrete Brownian Bridge $\vect{x}_j$, there are a couple ways to sample the times.
THe optimimal way to carry out samples

The rationale for also sampling over times is to allow further ensemble averaging over paths,
and times.  The original Gies method emphasized computing the total $\cT$ integral for each path.
For Dirichlet scalar world-lines, this was relatively tractable as it only requires finding the 
set of times when paths enter the body.  The integrals can be carried out immediately.
However, for the dielectric integrands we are using, the integrand must be re-evaluated for every time.
A large amount of computational effort must be expended on even a single path.  Furthermore, at the end 
of that effort one must still carry out a further ensemble average over paths.  
However, are free to treat the integrals over the starting position $\vect{x}_0$ and times $\cT$ 
in a Monte Carlo fashion as well.  This extends the number of independent paths we can average over, which 
is essential for good convergence.   

\subsection{First-touching time}

The simplest method which applies to TE/sojourn integrands is to find the first-touching time, $T_0$
when the renormalized integrand first takes on a non-zero value.  After that point $T_0$, the
integrand will slowly increase due to the increased sojourn value as more point pass into the 
surfaces.  However, there is an additional $1/T^{1+D/2}$ factor which damps these contributions out.

This suggests sampling from a probability distribution 
\begin{equation}
  P(T;T_0,m)= \frac{(m-1) T_0^{m-1}}{T^m}\Theta(T-T_0),
\end{equation}
where $m>1$ and $T_0>0$.  This requires being able to estimate the value of $T_0$.  For the example
of paths starting between parallel planes $T_0=\text{min}[\left(\frac{-d_1-x_0}{B_-}\right)^2,
\left(\frac{d_2-x_0}{B_+}\right)^2]$ where $d_1<x_0<d_2$.    

In order to sample from this distribution, we must invert 
\begin{equation}
  r=(m-1) T_0^{m-1}\int_{T_0}^T dt\, t^{-m}
\end{equation}
where $r\in [0,1]$ is a uniform random number and $T$ is the desired deviate.    
This can be straightforwardly inverted 
\begin{align}
  r&=(m-1) T_0^{m-1}\frac{1}{m-1}\left(\frac{1}{T_0^{m-1}}-\frac{1}{T^{m-1}}\right)\\
%  \left(\frac{T_0}{T}\right)^{m-1}&= (1-r)\\
 T= \frac{T_0}{u^{1/(m-1)}}
\end{align}
we $u=1-r$ is also a uniform random number. 

\subsection{Ensemble Average First-Touching Time}

However for integrands exploiting Feynman-Kac formulae, such as the TM integrand or 
the improved Dirichlet integrand, this strategy is less useful, since the integrand turns on softly.
In that case it is better to sample from a distribution
that better reflects the behaviour of the ensemble.
  A better distribution is 
\begin{equation}
  P_{\text{exp-T}}(T;T_0,s) = \frac{T_0^{s-1}}{\Gamma[s-1] T^s}e^{-T_0/T}\label{eq:expT},
\end{equation}
where $s>1$ and $T_0>0$.  In most cases of interest, $s=1+D/2$.  The exponential factor effectively models the finite touching 
probability for Brownian bridges starting at the origin to touch a planar surface a distance $d$ 
away in time $T$, $P_\text{touch}=e^{-2d^2/T}$.   This distribution can be used to sample times $T$
even for TE integrands.  In that case however, it is possible the generated path will not
touch all bodies and merely return zero.    

The normalization factor is determined by
\begin{align}
   1&= \int_0^\infty dT e^{-T_0/T} T^{-s}\\
%  &= \int_0^\infty ds\frac{1}{T_0} e^{-s}\left(\frac{T_0}{s}\right)^{-s+2}
  &= \eta T_0^{-s+1}\int_0^\infty d\tau\, e^{-\tau}\tau^{s-2}\\
 \implies \eta &= \frac{T_0^{s-1}}{\Gamma[s-1]}
\end{align}
where we changed variable to $s=T_0/T$, so $T=T_0/s$ and $ds=-T_0 dT/T^2$.  

% In order to sample from this distribution, we must invert 
% \begin{equation}
%   r=\int_0^T dt e^{-T_0/t}t^{-m}
% \end{equation}
% where $r\in [0,1]$ is a uniform random number and $T$ is the desired deviate.    
The probability distribution for $u=T_0/T$ is 
\begin{equation}
  P_{\text{exp}-1/T}(u;T0,s) = \frac{u^{s-2}}{\Gamma[s-1]}e^{-u}
\end{equation}
where we had to include $du/dT= -(T_0)/T^2=-1/(T_0u^2)$ for the change
of measure.  
Note that $u=T_0/T$ has the form of a Gamma distribution, where the distribution for   
\begin{equation}
  f(x) = \frac{x^{a-1} e^{-x/b}}{\Gamma(a)b^a}.  
\end{equation}
where $a>0$ and $b>0$ are the shape and scale parameters respectively.  
In order to develop deviates we follow Devroye~\cite{Devroye2003}
A sum of Gamma deviates $\sum_{i=1}g_i$ with shape parameters $\gamma(a_i)$, is also a Gamma deviate
with shape parameter $\sum a_i$ (pg 402 of Devroye).  
In particular, this means the sum of $k$ exponential deviates $e_i=-\log(u_i)$, yields a Gamma 
deviate gamma(k,1).  Furthermore if $z$ is normally distribed then $z^2/2$ obeys gamma$\left(\frac{1}{2},1\right)$.

So for (low) integer powers 
\begin{equation}
  T\sim \frac{T_0}{-\sum_{i=1}^{s}\log u_i},
\end{equation}
while for half-integer powers, 
\begin{equation}
  T\sim \frac{T_0}{-\sum_{i=1}^{\text{floor}(s)}\log u_i+z^2/2}.
\end{equation}
We will be predominantly interested in $s=1+D/2$ and $m=3/2$ to account for zero temperature
and non-zero temperature limits.  There are apparently better algorithms for moderate $m$,
such as might be required in large derivatives.

\section{Loop generation}

\subsection{TE}
\begin{enumerate}
\item Use v-loop algorithm from Gies.  Loops have Gaussian increments, and closed paths
\item (Derivation for loops)
\item Jacobian gives normalization.
\item Note connection to Brownian bridge SDE.  
\end{enumerate}

\subsection{TM}
\begin{enumerate}
\item TM potential must also be tracked along a given path.
( Explore the potential-only integrand to highlight the issues.)
\item TM potential takes on values $1<V_k<2$ outside a body, and $0<V_k<1$ when crossing
  or inside.  
\item Leads to large fluctuations, for even moderate $N$.  (Histogram of values, and scaling of variance)
\item Large fluctuations indicate a poor choice of probability distribution, and we should
  adjust our choice.
\item Two methods to incorporate the potential into the probability distribution.
  1) Adjust loop increments by sampling from $P_{TM}(x):= e^{-x^2/(2\Delta T)}e^{-\VTM}$.
  Integrand is piece-wise gaussian.  Can pick a particular Gaussian.  FOr differences of Gaussians,
  use rejection to sample the deviates.  
  2) Use regular Gaussian increments, but spawn new trajectories if acculumated potential 
  is large, and terminate if too small.  
\item Birth-death method is required for first method anyway, as there is still a potential
inf the form of the normalizations.  
\item Figure: Growth of fluctuations.
\item Figure: Histograms of normalization.
\end{enumerate}

\section{Gradient Estimation}

\begin{enumerate}
  \item Need gradients to compute forces, torques from potential.  Also need it directly
    for TM Casimir-Polder energy.
  \item Looking for corrections to gravity need curvature.  Similarly, estimating 
    (Cite Cornell expt) change in oscillation frequency.  Need two spatial derivatives.
    For TM potential, this is 4 spatial derivatives.  
\end{enumerate}


Let us consider the sensitivity of an expectation value $\dlangle f \drangle = \int dx f(x)P(x)$,
with respect to a parameter $\theta$, where $P(x)$ is the probability distribution.  
    The likelihood ratio method relies on differentiating the underlying probability distribution,
    and evaluating $\partial_\theta\dlangle f\drangle=\dlangle f(x)\partial_\theta\log P(x;\theta)\drangle$.  
    One can then estimate the gradient by generating 
    samples using the original probability distribution, while evaluating this new function.
    This can be readily applied to computing sensitivities of expectations with respect to Brownian motion~\cite{Broadie1996}.  
    A similar idea exploits the Malliavin calculus~\cite{Nualart2006}:
    In the Malliavin framework, one can estimate a sensitivity by evaluating $\dlangle f(x)\pi_\theta(x)\drangle$,
    where $\pi_\theta$ is the Malliavin weight~\cite{Fournie1999}.  
    The Malliavin calculus is essentially functional differentiation with respect to the Brownian motion.  
    One can derive the Malliavin weights by exchanging derivatives with respect to the parameter for
    derivatives with respect to the Brownian motion, and integrating by parts~\cite{Kohatsu-Higa2004}.  
    The Malliavin results can be recovered for Brownian motion if one combines the likelihood-ratio method with partial-averaging
    along the Brownian motion~\cite{Chen2007}.  In either case, one exchanges differentiation for
    evaluating a new reweighted function, which depends on the required derivatives, while still
    using the original paths.  


\subsection{Finite Differences}

\begin{enumerate}
  \item Simple to implement.  However, larger fluctuations, and biased. 
  \item Best to use common random numbers.  Also provides better error estimate.
  \item Need to balance choosing $N$, $ds$.  
  \item However difference-of-ensembles has better behaviour than 
    ensemble-of-differences.  Some ordering of operations noticeably superior.  
    Essentially passes finite difference onto $1/T^3$, which is nice and smooth.
\end{enumerate}

\subsection{Partial Averaging-Gaussian Paths}

\begin{enumerate}
  \item Consider how to evaluate Gradients with respect to source point of Gaussian 
    path integrals of the energy $\epsr$
    \begin{align}
      E =& -\frac{\hbar c\alpha_0}{2(2\pi)^{D/2}}\int \frac{d\cT}{\cT^{1+D/2-\alpha}}
      \biggdlangle \frac{1}{\langle\epsr\rangle}\biggdrangle\\
      =& -\frac{\hbar c\alpha_0}{2(2\pi)^{D/2}}\int \frac{d\cT}{\cT^{1+D/2-\alpha}}\int ds\,\frac{s^{\alpha-1}}{\Gamma[\alpha]}
      \biggdlangle e^{-s\cT \langle \epsr\rangle}\biggdrangle
    \end{align}
  \item If we use unshifted integration variables, $\vect{x}_k$, rather than the shifted, scaled
    Brownian motion variables, can differentiate immediately.
    Momentarily focus on just the path integral piece,
    \begin{equation}
      P = \biggdlangle e^{-s\cT \langle \epsr\rangle}\biggdrangle 
      = \int \prod_{j=1}^{N-1}dx_k \prod_{j=0}^{N-1}\frac{1}{(2\pi \Delta T)^{D/2}}e^{-(\vect{x}_{j+1}-\vect{x}_j)^2/2\Delta T-s\Delta T \epsr(\vect{x}_j)}.
    \end{equation}
    Main component are derivatives w.r.t. Gaussian.  Will neglect derivatives of the potential.  
    Our basic idea is to integrate out intermediate coordinates.  Under the assumption the 
    the integrands are approximately stable, or that the steps compose, we can average multiple
    steps.
  \item This is related to choosing a non-uniform loop with less resolution close to the beginning of the 
    loop.  This is justified via switching integration and differentation.  The Gaussian integrals 
    obviously compose.  The derivatives can just be carried out at the very end.  
  \item Derivatives of the Gaussian are Hermite polynomials:
    \begin{equation}
      \frac{d^n}{dx^n} e^{-x^2} = (-1)^n H_n(x)e^{-x^2}
    \end{equation}
    Scaling the variables to $x\rightarrow x/a$ yields
    \begin{equation}
      \frac{d^n}{dx^n} e^{-(x-\mu)^2/a^2} = a^{-n}(-1)^n H_n\big(\frac{x-\mu}{a}\big)e^{-(x-\mu)^2/a^2}
    \end{equation}

  %   \item Convolution formula.  Let us consider the convolution of Gaussians with a Hermite 
  %   polynomial.  This naturally emerges from integrating out a coordinate of the path integral
  %   \begin{equation}
  %     I = H_n\big(\frac{x-\mu_1}{\sqrt{2\sigma_1^2}}\big) \frac{e^{-(x-\mu_1)^2/(2\sigma_1^2)}}{\sqrt{2\pi \sigma_1^2}} 
  %     * \frac{e^{-(x-\mu_2)^2/(2\sigma_2^2)}}{\sqrt{2\pi \sigma_2^2}}.
  %   \end{equation}
  %   The convolution integral is most naturally carried out using the Fourier transform.
  %   \begin{align}
  %     f*g =& \int dx' f(x-x')g(x')\\
  %     = & \int dx' \frac{dk}{2\pi} \frac{dq}{2\pi} e^{ik(x-x')+iqx'} f(k)g(q)\\
  %     = & \int \frac{dk}{2\pi} e^{ikx} f(k)g(k)
  %   \end{align}
  %   The Fourier transform of the Gaussian is 
  %   \begin{align}
  %     \int dx e^{ikx}\frac{e^{-(x-\mu)^2/(2\sigma^2)}}{\sqrt{2\pi \sigma^2}} 
  %     &= \int dx' \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x'-ik/2)^2/(2\sigma^2)+ik\mu-\sigma^2k^2/2}\\
  %     &= e^{-\sigma^2 k^2/2+ik\mu}.
  %   \end{align}

  %   This can be straightforwardly extended to include Hermite polynomials via integration by parts.
  %   \begin{align}
  %     \int dx e^{ikx}(2\sigma^2)^{-n/2}(-1)^n H_n\big(\frac{x-\mu}{\sqrt{2\sigma^2}}\big)
  %       \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sqrt{2\pi \sigma^2}} 
  %     &= \int dx e^{ikx}\frac{d^n}{dx^n}\frac{e^{-(x-\mu)^2/(2\sigma^2)}}{\sqrt{2\pi \sigma^2}}\\
  %     &= (-ik)^n\int dx' \frac{e^{ik(x+\mu)-x^2/(2\sigma^2)}}{\sqrt{2\pi \sigma^2}}\\
  %     &= (-ik)^ne^{-\sigma^2 k^2/2+ik\mu}
  %   \end{align}

  %   The convolution of the Hermite-Gaussian, and a regular Gaussian is 
  %   \begin{align}
  %     I&=\int dx' (2\sigma_1^2)^{-n/2}(-1)^n H_n\left(\frac{x-x'-\mu_1}{\sqrt{2\sigma_1}}\right)
  %     \frac{e^{-(x-x'-\mu_1)^2/(2\sigma_1^2)-(x'-\mu_2)^2/(2\sigma_2^2)}}{2\pi \sigma_1 \sigma_2}\\
  %     &=\int dx' \int \frac{dk}{2\pi}\frac{dq}{2\pi} (-ik)^n e^{-ik(x-x'-\mu_1)}e^{-iq(x'-\mu_2)}
  %     e^{-\sigma_1^2k^2/2-\sigma_2^2q^2/2}\\
  %     &=\int \frac{dk}{2\pi}(-ik)^ne^{-ik(x-\mu_1-\mu_2)}e^{-(\sigma_1^2+\sigma_2^2)k^2/2}\\
  %     &=(-1)^n[2(\sigma_1^2+\sigma_2^2)]^{-n/2}H_n\bigg(\frac{x-\mu_1-\mu_2}{\sqrt{2(\sigma_1^2+\sigma_2^2)}}\bigg)
  %     \frac{e^{-(x-\mu_1-\mu_2)^2/[2(\sigma_1^2+\sigma_2^2)]}}{\sqrt{2\pi(\sigma_1^2+\sigma_2^2)}}
  %   \end{align}

  % \item Now to actually apply to integrating out Gaussians, starting from endpoint,
  %   we need various variable transformations.  Must switch between Hermite polynomial w.r.t $x_0$
  %   and convolution form.  
  %   \begin{align}
  %     & \partial_{0,i}^n e^{-(x_0-x_1)^2/(2T_1)-(x_0-x_{N-1})^2/(2T_{N-1})}\nonumber\\
  %     =& 
  %    \partial_{0,i}^n \exp\left[-\frac{T_1+T_{N-1}}{2T_1T_{N_1}}x_0^2-\frac{T_{N-1}x_1+T_1x_{N-1}}{T_1T_{N-1}}x_0
  %      -\frac{x_1^2}{2T_1}-\frac{x_{N-1}^2}{2T_{N-1}}\right]\\
  %    % =&\partial_{0,i}^n \exp\left[-\frac{T_1+T_{N-1}}{2T_1T_{N_1}}
  %    %   \left(x_0-\frac{T_{N-1}x_1+T_1x_{N-1}}{T_1+T_{N-1}}\right)^2
  %    %   +\frac{(T_{N-1}x_1+T_1x_{N-1})^2}{2T_1T_{N-1}(T_1+T_{N-1})}
  %    %   -\frac{x_1^2}{2T_1}-\frac{x_{N-1}^2}{2T_{N-1}}\right]\\
  %    =&(-1)^n \left(2\sigma^2_{1,N_1}\right)^{-n/2}
  %    H_n\bigg(\frac{x_0-z_{1,N-1}}{\sqrt{2}\sigma_{1,N-1}}\bigg)
  %      \exp\left[-\frac{(x_0-z_{1,N-1})^2}{2\sigma^2_{1,N-1}}-\frac{(x_1-x_{N-1})^2}{2(T_1+T_{N_1})}\right]\\
  %   \end{align}
  %   where 
  %   \begin{gather}
  %     \sigma_{1,N-1} = \sqrt{\frac{T_1+T_{N-1}}{2T_1T_{N_1}}}\\
  %     z_{1,N-1} = \frac{T_{N-1}x_1+T_1x_{N-1}}{T_1+T_{N-1}}
  %   \end{gather}
  % \item We really need 
  %   \begin{align}
  %     C=\int dx_1 \partial_0^n \frac{1}{(2\pi)^{3/2}\sqrt{T_1T_2T_{N_1}}}e^{-(x_0-x_1)^2/(2T_1)-(x_1-x_2)^2/(2T_2)-(x_0-x_{N-1})^2/(2T_{N-1})}
  %   \end{align}
    \item The simplest approach is to exchange integration and differentiation, then completing the square in $x_0$,
    and then carry out the derivatives
    \begin{align}
      C=&\partial_0^n\int dx_1  \frac{1}{(2\pi)^{3/2}\sqrt{T_1T_2T_{N_1}}}e^{-(x_0-x_1)^2/(2T_1)-(x_1-x_2)^2/(2T_2)-(x_0-x_{N-1})^2/(2T_{N-1})}\\
%      =&\partial_0^n \frac{1}{2\pi\sqrt{(T_1+T_2)T_{N_1}}}e^{-(x_0-x_2)^2/[2(T_1+T_2)]-(x_0-x_{N-1})^2/(2T_{N-1})}\\
      =&\partial_0^n \frac{1}{2\pi\sqrt{(T_1+T_2)T_{N_1}}}e^{-(x_0-\mu_2)^2/[2\sigma_2^2]-(x_2-x_{N-1})^2/(2[T_{N-1}+T_1+T_2])}\\
      =&(-1)^n(\sqrt{2}\sigma_2)^{-n/2}H_n\bigg(\frac{x_0-\mu_2}{\sqrt{2}\sigma_2}\bigg)
      \frac{1}{(2\pi)\sqrt{(T_1+T_2)T_{N_1}}}\nonumber\\
      &\times e^{-(x_0-x_2)^2/[2(T_1+T_2)]-(x_0-x_{N-1})^2/(2T_{N-1})},
    \end{align}
    where 
    \begin{gather}
      \mu_2 = \frac{T_{N-1}x_2+ (T_1+T_2)x_{N-1}}{T_1+T_2+T_{N-1}}\\
      \sigma_2^2 = \frac{(T_1+T_2)T_{N-1}}{T_{N-1}+T_1+T_2}
    \end{gather}
  \item Now evidently if we integrate out points symmetrically from the loop origin, and we 
    assume all $T_i=\Delta T$, then after integrated out $m$ steps we have 
    \begin{gather}
      \bar{x}_m = \frac{x_{m+1}+ x_{N-m-1}}{2}\\
      \sigma_2^2 = m\Delta T.
    \end{gather}
    The gradient of the path integral can then be approximately written as 
    \begin{align}
      C\approx
      &(-1)^n(\sqrt{2 T_m})^{-n/2}H_n\bigg(\frac{x_0-\bar{x}_m}{\sqrt{2 T_m}}\bigg)\nonumber\\
      &\times \frac{1}{(2\pi T_m)}e^{-(x_0-x_{m+1})^2/(2T_m)-(x_0-x_{N-m-1})^2/(2T_m)}.
    \end{align}
  \item We can estimate how many steps to integrate out based on the touching probability in time $T_m$.  
    The probability to touch a plane a distance $d$ away in time $T_m=m\Delta T = (m/N)T$ is 
    \begin{equation}
      P_{\text{touch}} = e^{-2d^2/T_m},
    \end{equation}
    which can be solved for $m$ if we require that $P_{\mathrm{touch}}$ does not exceed a threshold
    $10^{-\rho}$ with $\rho>0$  
    \begin{equation}
      \frac{m}{N} \le \frac{2d^2}{T\rho\ln(10)}.
    \end{equation}
    Note that as $N$ increases, the integration point approaches a constant fraction, even as $N$ increases.  
    This in effect reduces the fluctuations by $m^n$.  
    \item This is in effect choosing to make a loop with non-uniform steps, where in particular the first
    and last step are larger than the others.  We choose the size of those steps to be as 
    large as possible, while not interacting much with a the nearest surface.  

    \item Problem with adaptive choice?
\end{enumerate}

\subsection{General Method Near Surfaces}

     While estimating gradients for the stress-tensor it may be necessary to estimate 
    gradients while close to one surface.  For a renormalized energy, only loops that touch both
    surfaces will contribute.   In this case, the above approach based on integrating out Gaussians
    is of limited utility in reducing the fluctuations.  The assumption that the integrals are approximately
    Gaussian can only be explited for a very small number of steps.  
    However, if we assume there is a Feynman-Kac formula available taking into account the 
    interaction with a simple surface, then similar reasoning can be used.  In this case
    one integrates out further steps, since the Feynman-Kac formulae compose with one another,
    where now the threshold is must balance the path wandering sufficiently far that a simple 
    approximation.

    For example, consider using the Feynman-Kac formula for open loops near a Dirichlet surface, 
    \begin{equation}
      \dlangle e^{-V_D(x-d)}\drangle_{x_{j}\rightarrow x_{j+1}} 
      = \theta[(x_j-d)(x_{j+1}-d)]\left(1-e^{-2(x_j-d)(x_{j+1}-d)/T}\right)
    \end{equation}

\begin{enumerate}
  \item Gradients for atom
  \item Stress energy tensor 
\item Application to stress-tensor.  (Evaluate derivatives first, then take limit.)
  Problematic for stress-tensor values near one surface in 2-body scenarios. (Use exponential expressions
  to allow non-Gaussian integration.  Must just be stable distribution  
  Thus expact larger fluctuations near surfaces.  
  For example, stress-energy tensor requires 
  \begin{equation}
   T_{zz}=\lim_{\Delta\rightarrow 0} [\partial_{x_{CP}}^2 - \partial_{\Delta}^2]\dlangle \mathfrak{M}\drangle,
  \end{equation}
  where $\Delta = (x_N-x_0)/2, x_{CP} = (x_N+x_0)/2$.  Using the chain-rule, and employing the
  same differentiation this becomes. where $x_{CP}$ is the centre-point, and $\Delta$ is the separation.
  \begin{gather}
    x_{CP} =  \frac{1}{2}(x_{N}+x_0)\\
    \Delta = \frac{1}{2}(x_{N}-x_0)\\
    x_0  = x_{CP}-\Delta\\
    x_N = x_{CP}+\Delta
  \end{gather}
  Then
  \begin{align}
    \frac{\partial}{\partial x_{CP}} 
    %&= \frac{\partial x_0}{\partial x_{CP}}\frac{\partial}{\partial x_0}
    % +\frac{\partial x_N}{\partial x_{CP}}\frac{\partial}{\partial x_N}\\
&= \frac{1}{2}\frac{\partial}{\partial x_N}+\frac{1}{2}\frac{\partial}{\partial x_0}\\
  \frac{\partial}{\partial \Delta} 
%&= \frac{\partial x_0}{\partial \Delta }\frac{\partial}{\partial x_0}
%    +\frac{\partial x_N}{\partial \Delta}\frac{\partial}{\partial x_N}\\
&= \frac{1}{2}\frac{\partial}{\partial x_N}-\frac{1}{2}\frac{\partial}{\partial x_0}
  \end{align}
Then expanding these derivatives out (correct expression?)
\begin{align}
  T_{zz} &= \lim_{x_N\rightarrow x_0}\frac{1}{4}[(\partial_0+\partial_N)^2+(\partial_0+\partial_N)^2]G\\
  &= \lim_{x_N\rightarrow x_0}\frac{1}{2}[\partial_0^2+\partial_N^2]G
\end{align}

\end{enumerate}

\subsection{Hermite-Gaussian Sampling}

In order to apply the Hermite-Gaussian method to high order derivatives, it is best to 
sample from the combined Hermite-Gaussian.  The simplest approach uses Gaussian samples, weighted
by the appropriate Hermite-Gaussian function.  While this is acceptable for the first or second derivatives,
it breaks down for higher derivatives.  In particular, the Gaussian samples are most likely to be in a narrow
range $x\in(-3\sigma,3\sigma)$.  The Hermite-Gaussian oscillates in sign in this range, and
the dominant contributions come from the tails $x\sim \sqrt{n}\sigma$.  So for a finite sample,
one would see large fluctuations.  

The combined Gaussian steps for both $x_1, x_2$ which are pinned to start at $x_0$ is 
\begin{align}
  G &= \frac{1}{2\pi T_1T_2}\exp\left[-\frac{(x_0-x_1)^2}{2T_1}-\frac{(x_0-x_2)^2}{2T_2}\right]\\
  % &= \frac{1}{2\pi T_1T_2}\exp\left[-\frac{T_1+T_2}{2T_1T_2}x_0^2 +\left(\frac{x_1}{T_1}+\frac{x_2}{T_2}\right)x_0
  %   -\frac{x_1^2}{2T_1}-\frac{x_2^2}{2T_2}\right]\\
  &= \frac{1}{2\pi T_1T_2}\exp\left[-\frac{T_1+T_2}{2T_1T_2}\left(x_0 -\frac{x_1T_2+T_1x_2}{T_1+T_2}\right)^2
    -\frac{(x_1-x_2)^2}{2(T_1+T_2)}\right]
\end{align}
After differentiation w.r.t $x_0$, one finds 
\begin{equation}
  HG:= \partial_0^n G 
= \frac{1}{\sqrt{2\pi\sigma^2}\sqrt{2\pi(T_1+T_2)}} 
(-1)^n(2\sigma^2)^{-n/2}H_n\left(\frac{x_0-\mu_H}{\sqrt{2\sigma_H^2}} \right)
  \exp\left[-\frac{(x_0-\mu_H)^2}{2\sigma_H^2} - \frac{(x_1-x_2)^2}{2(T_1+T_2)}\right],
\end{equation}
where 
\begin{gather}
  \sigma_H^2:= \frac{T_1T_2}{(T_1+T_2)}\\
  \mu_H := \frac{x_1T_2+T_1x_2}{T_1+T_2}.
\end{gather}
In the event that $T_1=T_2$, this simplifies down to 
\begin{gather}
  \sigma_H^2:= \frac{T_1}{2}\\
  \mu_H := \frac{x_1+x_2}{2}.
\end{gather}
In order to make samples that are useful for closed paths one must include 
the normalization factor for the ``open'' Gaussian bridge that connects the end points $x_1$ and $x_2$
in time $T-T_1-T_2$.  
The total distribution for $x_1,x_2$ is 
\begin{align}
  HG'(x_1,x_2)&:=\frac{1}{\sqrt{2\pi\sigma^2}\sqrt{2\pi(T_1+T_2)}\sqrt{2\pi(T-T_1-T_2)} }
  (2\sigma^2)^{-n/2}H_n\left(\frac{x_0-\mu}{\sqrt{2\sigma^2}} \right)\nonumber\\
  &\hspace{0.5cm}\times\exp\left[-\frac{(x_0-\mu)^2}{2\sigma^2} - \frac{T(x_1-x_2)^2}{2(T-T_1-T_2)(T_1+T_2)}\right],
\end{align}
However, this is not normalized, and can even change sign.  
The samples must be taken from the absolute value of the integrand, with the sign of the Hermite-Gaussian
function weighting the samples.
The sign changes will naturally lead to cancellation.  In the presence of a potential $\Phi[x(t)]$,
the cancellation is not perfect and the result is non-zero.  
In order to best carry out the cancellation, and reduce fluctuations one should pair each sample of 
the Hermite-Gaussian with its negative.  

The probability distribution for the combined steps can be most naturally formulated in terms of the decoupled variables
\begin{align}
  \bar{x}:=\frac{x_1+x_2}{2}\\
  \Delta x := x_2-x_1.
\end{align}
The probability distributions for $\bar{x}$ and $\Delta x$ decouple as 
\begin{align}
  P_{HG,1}(\bar{x})&:=\frac{1}{\sqrt{2\pi\sigma_H^2}} 
  \bigg|H_n\bigg(\frac{x_0-\bar{x}}{\sqrt{2\sigma_H^2}} \bigg)\bigg|\nonumber\\
  &\hspace{0.5cm}\times\exp\left[-\frac{(x_0-\bar{x})^2}{2\sigma_H^2}\right]\\
  P_{HG,2}(\Delta x) &:=\sqrt{\frac{T}{2\pi(T_1+T_2)(T-T_1-T_2)}}
  &\hspace{0.5cm}\times\exp\left[- \frac{T(x_1-x_2)^2}{2(T-T_1-T_2)(T_1+T_2)}\right],
\end{align}
The numerical average can be computed as 
\begin{equation}
  I = \dlangle (-1)^n(2\sigma_H^2)^{-n/2}\eta_Hs_Hf\drangle_{HG},
\end{equation}
where $x_1,x_2$ are sampled from $P_{HG}$, and the remaining sub-bridge is sampled via
the open Gaussian probability distribution.  
where the normalization $\eta_H$ and sign factor $s_H$ are defined as 
\begin{align}
  s_H &:= \sgn\bigg[H_n\left(\frac{x_0-\mu_H}{\sqrt{2\sigma_H^2}} \right)\bigg] \\
  \eta_H &:=\frac{1}{\sqrt{2\pi T}}\int dy \frac{1}{\sqrt{2\pi\sigma_H^2}}
  \big|H_n\left(\frac{x_0-y}{\sqrt{2\sigma_H^2}} \right)\big|\nonumber\\
  &\hspace{0.5cm}\times\exp\left[-\frac{(x_0-y)^2}{2\sigma_H^2}\right],
\end{align}

It is best to numerically sample from distributions with unit variance, and rescale the deviates 
by the time.  In which case, $\bar{x}=h\sqrt{2\sigma_H}$, and $\Delta x = \sqrt{2T_1(T-2T_1)/T}z$,
where $z$ is a standard normal deviate as $h$ is sampled from 
\begin{align}
  P'_{HG,1}(h)&:=\frac{1}{\eta'_H\sqrt{\pi} }  \bigg|H_n\bigg(-h\bigg)\bigg|e^{-h^2}\\
  \eta'_H&=:=\int dh P'_{HG,1}(h).
\end{align}

The raw deviates will be denoted $h$ and $z$, where 
\begin{align}
  \frac{1}{2}(x_1+x_2) &= x_0 + \sqrt{\frac{T_1}{2}}h\\
  -x_1+x_2 &= \sqrt{\frac{2T_1(T-2T_1)}{T}}z
\end{align}
Then solving for $x_1, x_2$, 
\begin{align}
  x_1 & = x_0 + \left(\sqrt{\frac{T_1}{2}}h-\sqrt{\frac{T_1(T-2T_1)}{2T}}z\right)\\
  x_2 & = x_0 + \left(\sqrt{\frac{T_1}{2}}h+\sqrt{\frac{T_1(T-2T_1)}{2T}}z\right)
\end{align}
where $h' := h\sqrt{T_1/2}, z' := z\sqrt{\frac{T_1(T-2T_1)}{2T}}$.
\comment{Note these variables are scaled to agree with Dan, whose code seems to output 
  $h/\sqrt{2}$ naturally.  Again, he is treating $H_n(x)e^{-x^2}$ as the probability
  distribution.}


\section{Gradients for Casimir Energies}
\subsection{Surface Pinned Paths}
\label{sec:path-pinning}

\subsubsection{Force}
The force on a body follows from the gradient of the Casimir energy,
where the derivatives are taken with respect to the body's 
position.
For example, the components of the force on body $2$, expressed
in the basis $\hat{r}_i$, are
given by directional derivatives of the path-integral in Eqs.~(\ref{eq:spatial_path_integral}) and 
(\ref{eq:spatial_path_integralE}) with respect to
the components of the body position $\mathbf{R}_2$:
\begin{align}
  &F_{2,i}:=-\frac{\hbar c}{2(2\pi)^{D/2}}\int_0^\infty \!\!\frac{d\cT}{\cT^{1+D/2}}\change{\hat{r}_i\cdot\nablaR2} W
  \nonumber\\
   &\hspace{0.cm}=
   -\frac{\alpha\chi_2\hbar c}{2(2\pi)^{D/2}}
   \hat{r}_i\cdot\!\!
   \int_0^\infty \!\!\!\frac{d\cT}{\cT^{1+D/2}}   \!\int \!d\vect{x}_0\, 
  \Biggdlangle \frac{
  \big\langle 
  \delta(\sigma_2)\,\nabla\sigma_2\big\rangle}
  {\langle\epsr\rangle^{\alpha+1}}\Biggdrangle_{\!\vect{x}(t)}\!\!\!\!,
  \label{eq:forcepathint}
\end{align}
where $\sigma_2 = \sigma_2[\vect{x}(t)-\vect{R}_2]$ in this expression
\change{, and $\nablaR{i}$ denotes the gradient with
respect $\vect{R}_i$.}
The path integration can be simplified by viewing the $\delta$-function,
whose argument involves $\vect{x}(t)$, as a constraint on the
source point $\vect{x}_0$ of the paths.
Writing out the relevant part of the path-integral (\ref{eq:forcepathint}),
the $\delta$-function reduces the $D$-dimensional integration
over $\vect{x}_0$ to a $(D-1)$-dimensional integration over
the surface of body 2:
\comment{futzed with this - tried to restore}
\begin{align}
  &\int d\vect{x}_0  \Biggdlangle \frac{
  \big\langle 
  \delta\big(\sigma_2[\vect{x}(t)-\vect{R}_2]\big)\,\nabla\sigma_2[\vect{x}(t)-\vect{R}_2]\big\rangle}
  {\langle\epsr\rangle^{\alpha+1}}\Biggdrangle_{\vect{x}(t)} \nonumber\\
  &\hspace{0.5cm}= 
  \int d\vect{x}_0  \Biggdlangle \frac{
  \delta\big(\sigma_2[\vect{x}_0-\vect{R}_2]\big)\,\nabla\sigma_2[\x0-\vect{R}_2]}
  {\langle\epsr\rangle^{\alpha+1}}\Biggdrangle_{\vect{x}(t)} \nonumber\\
  &\hspace{0.5cm}= 
  \oint_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}^{}
   \hspace{-8ex}dS\hspace{5ex}\biggdlangle 
  \frac{\nabla\sigma_2(\x0-\vect{R}_2)}
  {\langle\epsr\rangle^{\alpha+1}|\nabla\sigma_2(\x0-\vect{R}_2)|}\biggdrangle_{\vect{x}(t)},
  \label{eq:delta-normal}
\end{align}
where the final integral (over path source points $\vect{x}_0$)
is a surface integral over the 
surface of body 2.  The first equality here can be understood
in terms of a discrete path (i.e., in a ``time-slicing'' regularization
of the path-integral), where the path average in the numerator
amounts to a sum of terms, each involving a $\delta$-function
involving a path coordinate $\vect{x}_j$.  In the average over all
paths and sum over all source points $\vect{x}_0$,
the $\delta$-function is equivalently a function of the
source point, since $\vect{x}_j$ is the source point
of another equivalent path.
The second equality of Eqs~(\ref{eq:delta-normal})
follows from an application of the H\"ormander formula
(see Eq.~(22) in Ref.~\cite{Mackrory2016}).
The renormalized force vector can be found by summing over all force components and subtracting 
the corresponding single-body energy,
\change{used ``limit'' on oint to get limits underneath integral}
\begin{align}
  \vect{F}_{2}&=
  -\frac{\alpha\chi_2\hbar c}{2(2\pi)^{D/2}}
\int\limits_0^\infty \!\frac{d\cT}{\cT^{1+D/2}}    
\hspace{-3ex}
 \oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}^{}
  \hspace{-4ex} dS\hspace{1ex} 
  \hat{n}_2(\vect{x}_0) \nonumber\\
  &\hspace{0.5cm}\times 
  \Biggdlangle\frac{1}{\langle\epsilon_{\mathrm{r},12}\rangle^{\alpha+1}}-\frac{1}{\langle\epsilon_{\mathrm{r},2}\rangle^{\alpha+1}}
  \Biggdrangle_{\vect{x}(t)},
  \label{eq:pinning_force}
\end{align}
where the unit-normal vector for the surface of body 2 is defined by
\begin{equation}
  \hat{n}_2(\x0) := -\frac{\nabla \sigma_2(\x0-\vect{R}_2)}{|\nabla \sigma_2(\x0-\vect{R}_2)|}.
\end{equation}
Qualitatively, the Casimir force on a body arises from 
paths that begin (and end) on its surface, with a vector
path weight
in the direction of the surface normal at the path source point. 
Although the surface of an arbitrary body involves surface normals
pointing in all directions, each surface normal obtains a 
different geometry-dependent
weight via the path ensemble.  The result is, in general,
a nonzero net force.

\subsubsection{Potential Curvature}

This method can be easily extended to the second derivative of the worldline energy, which 
computes the potential curvature,  
\begin{equation}
  C_{ij} := (\hat{r}_i\cdot \nablaR2)(\hat{r}_j\cdot \nablaR2)E.
\end{equation}
For a dielectric describing two bodies, the derivatives with respect to $\vect{R}_2$ in direction $\hat{r}_i$, can be rewritten 
in terms of derivatives with respect to the first body's center $\vect{R}_1$, and the loop coordinates $\vect{x}_k$
\begin{align}
  \nablaR2\langle \epsr\rangle  
  =& \bigg(\sum_{k=1}^N\nablaxk-\nablaR1\bigg)\nonumber\\
  & \times[\langle \epsilon_1(\vect{x}-\vect{R}_1)\rangle+\langle\epsilon_2(\vect{x}-\vect{R}_2)\rangle],
% \frac{\partial}{\partial R_{2,i}}\langle \epsr\rangle  
%   =& \bigg(\sum_{k=1}^N\frac{\partial}{\partial {x}_{k,i}}-\frac{\partial}{\partial R_{1,i}}\bigg)\nonumber\\
%   & \times[\langle \epsilon_1(\vect{x}-\vect{R}_1)\rangle+\langle\epsilon_2(\vect{x}-\vect{R}_2)\rangle],
  \label{eq:shift_derivative}
\end{align}
where $\nablaxk$ is the gradient of the path position $\vect{x}_k$.    
The first derivative can be carried out as before,
% \begin{align}
%   C_{ij} = &
% \frac{\alpha\chi_2\hbar c}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}
% \int d\vect{x}_0 \nonumber\\
% &\hspace{0.05cm}\times\biggdlangle 
% \hat{r}_{i}\!\cdot\!\bigg(\!\sum_k\nablaxk - \nablaR{1}\!\bigg)
%   \!
%   \bigg[
%   \frac{\hat{r}_{j}\cdot\langle \delta(\sigma_2)\nablaR{2}\sigma_2\rangle}
%   {\langle\epsr\rangle^{\alpha+1}}\bigg]\biggdrangle_{\vect{x}(t)}.
% \end{align}
\begin{align}
  C_{ij} = &
\frac{\alpha\chi_2\hbar c}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}
\int d\vect{x}_0 \nonumber\\
&\hspace{0.05cm}\times\biggdlangle 
\hat{r}_{i}\!\cdot\!\bigg(\!\sum_k\nablaxk - \nablaR{1}\!\bigg)
  \!
(\hat{r}_{j}\cdot\nablaR{2})
  \frac{1}
  {\langle\epsr\rangle^{\alpha+1}}\biggdrangle_{\vect{x}(t)}.
\end{align}
It is possible to integrate by parts on the gradients $\nablaxk$, 
which then act on the Gaussian probability density,
 and which yields a term proportional to $\sum_{k}(2\vect{x}_k-\vect{x}_{k+1}-\vect{x}_{k-1})$.
This sum of path increments vanishes for closed paths, and thus this term can be dropped.  
The remaining gradient in $\nablaR{1}$ can be straightforwardly evaluated, which yields 
a second independent path-averaged $\delta$-function.  
% and using Eq.~(\ref{eq:delta-normal}), there are now two path-averaged delta-functions which 
% pin the paths to lie on both the first and second surfaces
One $\delta$-function can be manipulated as in Eq.~(\ref{eq:delta-normal}) to constrain the the paths to start on
the first body, while the second $\delta$-function-average pins another point of the path to lie on the second body
and then takes a further averages over which point is pinned.
The resulting expression for the potential curvature is 
\comment{kept $\vect{x}(t)$ subscript on ensemble average: Actually what convention do we want with these?
Sometimes we're adding them, other times we are not}
\begin{align}
  C_{ij}&=
  \frac{\alpha(\alpha+1)\chi_1\chi_2}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}
  \oint\limits_{\sigma_1(\vect{x}_0-\vect{R}_1)=0}^{}
   \hspace{-4ex} dS\hspace{1ex}   \nonumber\\
  &\hspace{0.5cm} \times\sum_{k=1}^{N-1}\frac{1}{N} \biggdlangle  \mathcal{G}(\vect{x}_0,\vect{x}_k,k,\cT)\nonumber\\
  &\hspace{1cm}\times\frac{[\hat{r}_{i}\cdot\hat{n}_1(\vect{x}_0)][\hat{r}_{j}\cdot\hat{n}_2(\vect{x}_k)]}
  {\langle \epsilon_{\mathrm{r},12}\rangle^{\alpha+2}}     \biggdrangle_{\vect{x}(t)|\sigma_2(\vect{x}_k-\vect{R}_2)=0}.
  \label{eq:potential_curvature}
\end{align}
where 
$\dlangle \cdots\drangle_{\vect{x}(t)|\sigma_2(\vect{x}_k-\vect{R}_2)}$ is the ensemble
average over discrete paths $\vect{x}(t)$ subject to the constraint that $\sigma_2(\vect{x}_k-\vect{R}_2)=0$,
and 
\begin{equation}
  \mathcal{G}(\vect{x}_0,\vect{x}_k,k,\cT)=\frac{e^{-N^2(\vect{x}_0-\vect{x}_k)^2/(2 k(N-k) \cT)}}{[2\pi  \cT k (N-k)/N^2]^{D/2}}
  \label{eq:Gauss_normalization}
\end{equation}
is the Gaussian normalization factor from fixing $\vect{x}_k$ after $k$ steps, and returning to $\vect{x}_0$
in $N-k$ steps. 
There is no need for any further renormalization, since this expression is only non-zero in the presence 
of both bodies, and $\mathcal{G}$ exponentially cuts off terms at small $\cT$.    

% \begin{shaded}
%   Let us double check that normalization constant.  This can be verified by H\"ormander's expression for 
%   the delta function.   
%   \begin{equation}
%     \int \prod_{k=1}^N dq_k\, f(\vect{q}) \delta[h(\vect{q})]= 
%     \oint_{h^{-1}(0)} dS f(\vect{q})\frac{1}{\sqrt{|\nabla h(\vect{q})|^2}}
%   \end{equation}
  
%   We have previously applied this in the context of the path normalization.  
%   In that example, we treat the integration variables $q_k$ are the path increments, $\delta x_k$.
%   Then the path-closure condition yields
%   \begin{align}
%     &\int \prod_{k=0}^{N-1} d(\Delta x_k) \delta(\sum_k \Delta x_j)f(\{\Delta x_k\})
%     \prod_{j=0}^{N-1}\frac{1}{\sqrt{2\pi\Delta T}}e^{-\Delta x_k^2/(2\Delta T)}\nonumber\\
%    &= \frac{1}{\sqrt{2\pi\Delta T N}}\oint_{\sum_k\Delta x_k=0} \prod_{k=0}^{N-2} d(\Delta x_k)f(\{\Delta x_k\})
%     \prod_{j=0}^{N-2}\frac{1}{\sqrt{2\pi\Delta T}}e^{-\Delta x_k^2/(2\Delta T)},
%   \end{align}
%   which leads to the familiar $1/\sqrt{2\pi T}$ normalization for Brownian bridges.  

%   In this case we want fixing to the surface, and also d-dimensional path closure.  
%   \begin{align}
%     S=&\int d\vect{x}_0\int \prod_{k=0}^{N-1} d(\Delta \vect{x}_k) \delta[\sigma_1(\vect{x}_0)]
%     \delta[\sigma_r(\sum_{j=1}^k \Delta \vect{x}_j)]
%     \delta^{(d)}[\sum_{j} \Delta \vect{x}_j]
%     f(\{\Delta \vect{x}_k\})
%     \prod_{j=1}^N\frac{1}{\sqrt{2\pi\Delta T}}e^{-\Delta \vect{x}_k^2/(2\Delta T)},
%   \end{align}  
%   where $\vect{x}_k = \vect{x}_0+\sum_{j=0}^{j-1}\Delta\vect{x}_0$, $\Delta \vect{x}_k=\vect{x}_{k+1}-\vect{x}_k$.
%   (Heuristically I argued I get an surface integral over intermediate positions, weighted by 
%   the Gaussian probabilities to go to a point on the surface in $k$ steps, and then return in $N-k$.)

%   So we can evaluate the integrals over $\vect{x}_0$ and $\Delta \vect{x}_{N-1}$ immediately.  
%   \begin{align}
%     S=&\frac{1}{\sqrt{2\pi T}}\oint_{\sigma_1(\vect{x}_0)=0} dS_0\frac{1}{|\vect{n}_1(\vect{x}_0)|}
%       \int_{\sum_{k}\Delta x_k=0} \prod_{k=0}^{N-2} d(\Delta \vect{x}_k) 
%     \delta[\sigma_b(\sum_{j=1}^k \Delta \vect{x}_j)]
%     \prod_{j=1}^N\frac{1}{\sqrt{2\pi\Delta T}}e^{-\Delta \vect{x}_k^2/(2\Delta T)}
%     f(\{\Delta \vect{x}_k\}),
%   \end{align}  
% \end{shaded}



\begin{shaded}
Attempt at Alternative double-pinning notation. 
\begin{align}
  C_{ij}&=
  \frac{\alpha(\alpha+1)\chi_1\chi_2}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}\int_0^\cT \frac{d\tau}{\cT}\nonumber\\
  &\times
  \oint\limits_{\sigma_1[\vect{x}(0)]}^{}   \hspace{-2ex} dS
  \oint\limits_{\sigma_2[\vect{x}(\tau)]}^{}  \hspace{-2ex} dS'\hspace{1ex}   
\mathcal{G}'[\vect{x}(0),\vect{x}(\tau),\tau,\cT]\nonumber\\
  &\hspace{1cm}\times\biggdlangle\frac{\{\hat{r}_{i}\cdot\hat{n}_1\big[\vect{x}(0)\big]\}
    \{\hat{r}_{j}\cdot\hat{n}_2\big[\vect{x}(\tau)\big]\}}
  {\langle \epsilon_{\mathrm{r},12}\rangle^{\alpha+2}}     \biggdrangle_{\vect{x'}(t); \vect{x'}(\tau)},
  \label{eq:potential_curvature}
\end{align}
where the ensemble average is over paths $\vect{x'}(\tau)$ that are 
pinned to lie on surface $\sigma_1, \sigma_2$ at path positions
$\vect{x}(0), \vect{x}(\tau)$ respectively.  The Gaussian normalization is 
\begin{equation}
  \mathcal{G}'[\vect{x}(0),\vect{x}(\tau),\tau,\cT]=\frac{\sqrt{\cT}}{\sqrt{2\pi  \tau(\cT-\tau)}}
  e^{- \cT[\vect{x}(0)-\vect{x}(\tau)]^2/[2 \tau(\cT-\tau)]}
\end{equation}
\begin{itemize}
\item Use $\vect{x'}$ to distinguish these contrained paths?
\item Double subscripts on ensemble average to denote paths going from $x(0)$ to $x(\tau)$?
\item Use only $\sigma_1[\vect{x}]$ to denote surface we seek?  Current notation reflects desire
ato use sign $\sigma$ in step functions for dielectric, so surface is $\sigma=0$.  Also want
to emphasize body center $\vect{R}_i$ for rigid translations.
\item Problems with later occupation number results, where summing over terms leads to explicit factors of 
$N$.  
\end{itemize}
\end{shaded}

\subsubsection{Torque}
The torque on a body can be found from the first order variation in the energy as that body is
infinitesimally rotated about some axis.  
For concreteness, consider perturbing the dielectric by rotating the second body about its center
by angle $\phi$ about axis $\hat{m}$,
% \begin{equation}
%   K_m = -\partial_\phi W,
% \end{equation}
%where the perturbed dielectric is 
\begin{equation}
  \epsr(\vect{x}) = 1+\chi_1\theta[\sigma_1(\vect{x}-\vect{R}_1)]
  +\chi_2\theta\big\{\sigma_2[\mathcal{R}(\phi)(\vect{x}-\vect{R}_2)]\big\}.
\end{equation}
% where the second body has undergone an infinitesimal rotation 
% by an angle $\phi$ about an axis $\hat{m}$ about its center.  
The infinitesimal rotation matrix is given by 
\begin{equation}
  \mathcal{R}_{ij}(\phi) = \delta_{ij} - m_k\epsilon_{ijk}\phi ,
\end{equation}
where $\delta_{ij}$ is the Krocker delta, and $\epsilon_{ijk}$ is the antisymmetric Levi-Civita tensor. 
Throughout this subsection there are implicit sums over repeated indices.    
The torque for a rotation about axis $\hat{m}$ can be written as $K_m:=\hat{m}\cdot\vect{K}=-\partial_\phi E$.
The $\phi$-derivative only acts on the path-averaged dielectric part of the energy integral,
\begin{align}
  \partial_\phi\langle\epsilon\rangle&=
  \chi_2\langle \partial_\phi\mathcal{R}_{ij}(\phi)(\vect{x}-\vect{R}_2)_j[\hat{r}_i\cdot\nabla\theta(\sigma_2)]\rangle\nonumber\\
  &=-\chi_2\langle m_k\epsilon_{kij}(\vect{x}-\vect{R}_2)_j[\hat{r}_i\cdot\nabla\theta(\sigma_2)]\rangle\nonumber\\
  &=\chi_2\hat{m}\cdot\langle (\vect{x}-\vect{R}_2)\wedge\nabla\theta(\sigma_2)\rangle,
\end{align}
where we used the form of the infinitesimal rotation to write the result as a cross-product 
via $(\vect{a}\wedge\vect{b})_i=\epsilon_{ijk}a_jb_k$.  
This derivative can be directly substituted into the full torque path-integral, 
and similar manipulations to Eq.~(\ref{eq:delta-normal}) can be carried out
to pin the paths to start on the surface of the second body.
In addition, given the form of $\partial_\phi\langle\epsr\rangle$ the full torque $\vect{K}$
can be found by identifying $\partial_\phi E=\hat{m}\cdot\vect{K}$.  
The full renormalized torque worldline path-integral is 
\begin{align}
  \vect{K} &= \frac{\alpha\hbar c\chi_2}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}} 
  \hspace{-3ex}
  \oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0} 
   \hspace{-4ex} dS\hspace{1ex}\!\big[(\vect{x}_0-\vect{R}_{2})\! \wedge \!\hat{n}_2(\vect{x}_0)\big]   \nonumber\\
  &\hspace{0.5cm}\times\biggdlangle 
\frac{1}{\langle \epsilon_{\mathrm{r},12}\rangle^{\alpha+1}}
  -\frac{1}{\langle \epsilon_{\mathrm{r},2}\rangle^{\alpha+1}}\biggdrangle_{\vect{x}(t)}.
\end{align}
This has the intuitive interpretation of finding the total torque on the body by 
integrating over its surface and taking the cross-product of the vector from the body's center to a surface
element with the force density at that surface element.  

\subsubsection{Casimir--Polder Force}

We briefly note that an alternative expression for the Casimir--Polder force on an atom near a surface
can be found in analogy to the potential curvature in Eq.~(\ref{eq:potential_curvature}).
The force on the atom is $F\subCPi = -\hat{r}_i\cdot\nabla_{\rA}E$.
  In Sec.~\ref{sec:partial_average} we took the derivatives of the path-integral immediately,
  with the dominant contribution coming from the Gaussian probability distribution.  
  Alternatively, one can change the coordinates to $\vect{x}(t)=\rA+\vect{y}(t)$, where 
  $\vect{y}(t)$ is a Brownian bridge starting and returning to the origin, $\vect{y}(0)=\vect{y}(\cT)=0$,
  and then take the desired gradients.
  The resulting force expression is
\begin{align}
  F_i\supTE \!=&\! -\frac{\hbar c\alpha_0}{4(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}\biggdlangle 
  \hat{r}_i\!\cdot\!\nabla_{\rA}\langle\epsr\rangle^{-3/2}
  \biggdrangle_{\vect{y}(t)},
\end{align}
where this path-integral considers the change in energy as the whole path is translated,
while the results in Sec.~\ref{sec:partial_average}
correspond to shifting only the origin of the path, while keeping the rest of the path fixed.
The derivatives can be carried out, which for piece-wise constant media create delta-functions.
In analogy with the potential curvature, since the starting point is fixed, it is necessary to 
average over pinning other path points to lie on the dielectric surface for each of the bodies.  
The Casimir--Polder force, after summing over all force components, is 
\begin{align}
  \vect{F}\supTE\subCP&=-\frac{3\hbar c\alpha_0}{8(2\pi)^{D/2}}
  \sum_{b=1}^{N_b}\sum_{k=1}^{N-1}\frac{\chi_b}{N}\intzinf \frac{d\cT}{\cT^{1+D/2}}
   \nonumber\\
   &\hspace{0.5cm} \times \biggdlangle \mathcal{G}(\vect{x}_A,\vect{x}_k,k,\cT)
   \frac{\hat{n}_b(\vect{x}_k)}
  {\langle \epsr\rangle^{5/2}}     \biggdrangle_{\vect{x}(t)| \sigma_b(\vect{x}_k-\vect{R}_b)=0},
\end{align}
where we have reverted to using $\vect{x}(t)$, $\mathcal{G}$ is given by 
Eq.~(\ref{eq:Gauss_normalization}), and $b$ indexes each of the $N_b$ dielectric bodies.
In this method the paths are constrained to touch the bodies, which must be taken into account numerically
by averaging over paths where each index that is constrained.
In constrast, the Hermite-Gaussian method discussed in Sec.~\ref{sec:partial_average} 
uses the same paths regardless of the dielectric background.
While the path-pinning method requires more complicated path generation,
it does not suffer from diverging fluctuations as the path resolution is increased.
The Gaussian factor $\mathcal{G}$ exponentially suppresses contributions from pinning small indices $k$,
which would be the problematic terms as $\Delta \cT\rightarrow 0$, 
and thus this method does not require careful handling as $N$ increases.  

\subsection{Occupation Number}
\label{sec:occupation}

The preceding methods offer an intuitive picture of the Casimir force,
however they are poorly behaved in the strong-coupling limit.  
For a typical path of $N$ steps pinned to the surface, approximately half 
of the path will lie inside the body.  For $\chi\gg N$, the denominator $\langle\epsr\rangle^{-1/2}$ dominates
the integrand, so the estimated derivatives tend to zero as $\chi^{-1/2}$ for almost all paths.  
Only rare paths which start on the surface, but do not enter the bulk of the body will contribute significantly.  
As a result the estimated force goes to zero in the strong-coupling limit.
In this section we develop alternative expressions which are better behaved in the strong-coupling
limit and makes direct contact with prior work on Dirichlet worldlines.  

The spatial path-integral can be written in exponential form via the Gamma function,
\begin{align}
  W &= \frac{1}{\Gamma[\alpha]}\int d\vect{x}_0 \int ds\, s^{\alpha-1}e^{-s}\nonumber\\
  &\hspace{0.5cm}\times\bigdlangle e^{-\langle \sum_b\chi_b\theta_b(\vect{x})\rangle}
  - e^{-\sum_b\chi_b\theta_b(\vect{x}_0)}\bigdrangle_{\vect{x}(t)}\label{eq:W_exp2}
\end{align}
where we have introduced a shorthand notation $\theta_b(\vect{x}) = \theta(\sigma_b[\vect{x}-\vect{R}_b])$.
% This exponential form is similar to the approach required to account for non-zero temperature and material
% dispersion~\cite{Mackrory2016}.  The exponential expression also places the integrand in a suitable form 
% to exploiting relevant Feynman-Kac formulae.  
The exponential spatial path-integral for two bodies, after the single-body expressions have been factored out can 
be factorized as 
\begin{align}
  W &= \frac{1}{\Gamma[\alpha]}\int d\vect{x}_0 \int ds\, s^{\alpha-1}e^{-s}\nonumber\\
  &\hspace{0.5cm}\times\Bigdlangle 
  (e^{-\langle \chi_1\theta_1(\vect{x})\rangle}-1)(e^{-\langle \chi_2\theta_2(\vect{x})\rangle}-1)\nonumber\\
  &\hspace{1.25cm}
  -(e^{- \chi_1\theta_1(\vect{x}_0)}-1)(e^{-\chi_2\theta_2(\vect{x}_0)}-1)\Bigdrangle_{\vect{x}(t)}\label{eq:W_exp2}
\end{align}
\comment{Pretty sure a Gies paper does a similar factorization.}
%\begin{align}
  % &e^{-s}[1+e^{-s\langle \chi_1(\vect{x})+\chi_2(\vect{x})\rangle } 
  % - e^{-s\langle \chi_1(\vect{x})\rangle }-e^{-s\langle \chi_2(\vect{x})\rangle}]\nonumber\\
The exponential of the path-averaged potential can be written as a product of potentials 
for each increment, and the potential can be simplified for step-function dielectrics,
\begin{align}
  e^{-s \langle\chi_b\theta_b(\vect{x})\rangle}= \prod_{k=0}^{N-1}
  \left[\bar{\theta}_{b,k}+\theta_{b,k}e^{-s\chi_b/N}\right].\label{eq:exp_step_limit1}
\end{align}
where $\theta_{b,k} := \theta[\sigma_b(\vect{x}_k-\vect{R}_b)],$ 
and $\bar{\theta}_{b,k}:=1-\theta_{b,k}$.  This way of regularizing the surface leads to 
a different representation for the gradients of the Casimir energy.
In this section, we are assuming the step-function is arbitrarily sharp, and taking that limit before any gradients are taken.  
The gradient for a single position can be computed from Eq.~(\ref{eq:exp_step_limit1}) as
\begin{align}
  [\nablaxk e^{-s \chi_b\theta_{b,k}/N}]_{\text{occupation}} 
=& \delta_{b,k}(e^{-s\chi_b/N}-1)\vect{n}_b(\vect{x}_k)
    \label{eq:occupation-grad}
\end{align}
where $\delta_{b,k} := \delta[\sigma_b(\vect{x}_k-\vect{R}_b)]$ and the un-normalized surface normal is
$\vect{n}_b(\vect{x}_k):=\nabla\sigma_{b}(\vect{x}_k-\vect{R}_b)$.
The results in Sec.~\ref{sec:path-pinning} can be recovered if the gradient is 
taken before the step-function limit is taken.  
In essence, that calculation assumed the step was the limit of a smooth sigmoidal function 
and that the path had arbitrarily fine resolution on the scale over which the step-function jump occured,
so that the gradient follows from applying the chain-rule, 
\begin{align}
  [\nablaxk e^{-s \chi_b\theta_{b,k}/N}]_{\text{pinning}} 
 =& \frac{s\chi_b}{N} \delta_{b,k}e^{-s \chi_b\theta_{b,k}/N}\vect{n}_b(\vect{x}_k)
  \label{eq:pinning-grad}
\end{align}
However, throughout this section we will treat using Eq.~(\ref{eq:occupation-grad}) to evaluate gradients,
and later show how the earlier results are recovered.

The force on the second body can be computed by differentiating
the energy with respect to the body position $\vect{R}_2$.  The spatial part of the force integral
can be defined as
\begin{align}
  W_{F,2} :=& -\nablaR{2}W\\
  =& -\frac{1}{\Gamma[\alpha]}\int d\vect{x}_0\int ds\,s^{\alpha-1}e^{-s}\biggdlangle 
  \big(e^{-s\chi_2/N}-1\big)\nonumber\\
  &\times\sum_{j=0}^{N-1}[\vect{n}_2(\vect{x}_j)\delta_{2,j}
  \prod_{k\ne j}\left(\bar\theta_{2,k}+\theta_{2,k}e^{-s\chi_2/N}\right)\nonumber\\
  &\times\bigg[1-\prod_{n=0}^{N-1}\left(\bar{\theta}_{1,n}+\theta_{1,n}e^{-s\chi_1/N}\right)\bigg]\biggdrangle_{\vect{x}(t)}.
\end{align}
where the constant term has zero derivative.
The $s$-integral can be carried out more easily if the integrand is re-arranged into terms with 
a definite number of points $n$ inside each body $b$.  
We define the indicator functions
\begin{align}
  \I[b]0&:= \prod_{j=0}^{N-1}\bar{\theta}_{r,j}\\
  \I[b]n&:= \sum_{j_1=1}\sum_{j_2>j_1}\cdots\sum_{j_{n}>j_{n-1}}\theta_{b,j_1}\theta_{b,j_2}\cdots\theta_{b,j_n},
 \quad n\ge 1,
\end{align}
where $\I[b]n=1$ when there are exactly $n$ points inside body $b$, and zero otherwise;  
there are $n$ sums over indices $j_{n}$, each of which terminates at $j_n=N$.  
There are further restrictions on which of these terms contribute in the integrand.
Due to the presence of the $\delta$-functions, only $N-1$ points are free to 
enter the bodies.  This further implies that the number of points inside both bodies must be less than $N-1$. 
Finally, due to the renormalization only paths with at least one point inside the first body contribute.  
Using the indicator functions, the re-arranged spatial path-integral for the force is 
\begin{align}
  W_{F,2} =& (-1)\int d\vect{x}_0\int ds\,s^{\alpha-1}e^{-s}\biggdlangle \sum_{j=0}^{N-1}\hat{n}_2(\vect{x}_j)\delta_{2,j}\nonumber\\
  &\times\sum_{n=0}^{N-1}
  \big(e^{-s(n+1)\chi_2/N}-e^{-sn\chi_2/N}\big)\I[2]n\nonumber\\
  &\times \sum_{m=1}^{N-n-1}\big(1- e^{-s m \chi_1/N} \big)\I[1]m
  \biggdrangle_{\vect{x}(t)},
\end{align}
The $s$-integral can be carried out term by term, the $\delta$-function can be used to pin paths onto the surface,
and the cyclic-permutation invariance of the path can be used to remove the path-average over pinning, 
as in Eq.~(\ref{eq:delta-normal}).
The full force path-integral is given by 
\begin{align}
  \vect{F}_2 =& (-1)\frac{\hbar c N}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}\oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}  \hspace{-4ex} dS
\nonumber\\
  &\hspace{0.5cm}\times \sum_{n=0}^{N-1}\sum_{m=1}^{N-n-1}\bigdlangle\hat{n}_2(\vect{x}_0)
  \I[1]m\I[2]n f_{m,n}\bigdrangle_{\vect{x}(t)}
  \label{eq:occupation_force}
\end{align}
where the material dependence is carried by 
\begin{align}
  f_{m,n}&:=c_{m,n}-c_{m,n+1}-c_{0,n}+c_{0,n+1},\\
  c_{m,n} &:= \bigg( 1 + \frac{m\chi_1+n\chi_2}{N}\bigg)^{-\alpha},
\end{align}
which come from computing the change in the renormalized energy integrand as another point enters
the second body.  When $\chi_2/N\ll 1$, $f_{m,n}$ can be to leading order in $\chi_2/N$, 
to recover our earlier results for the force.
% The ensemble average $\dlangle\cdots\drangle_{\vect{x}(t)|\sigma_{2}(\vect{x}_j-\vect{R}_2)}$ should be understood to only allow 
% paths where $\vect{x}_j$ is restricted to lie on the surface $\sigma_2(\vect{x}_j-\vect{R}_2)=0$.
% This also applies to the term where $\vect{x}_0$ is fixed, and in that case $\vect{x}_0$ should be restricted to the surface.

In this result the indicator functions carry the position dependence based on whether a given number of points are 
within each body, while $f_{n,m}$ carries the dependence on material properties based on the number of points inside each body.  
This expression is well-behaved in the $\chi\rightarrow\infty$ limit, where only 
the $n=0, m>0$ terms contribute.  In the strong-coupling limit, the main contribution to the 
force comes from paths that just graze the second surface, while also entering the first body.  

For completeness we note the analogous expressions for the torque and potential curvature.  
The manipulations and reasoning used in Sec.~\ref{sec:path-pinning} for the torque and potential curvature
apply here --- the only difference is the form chosen for the gradient, 
and using the indicator functions in the integrand.  
The torque path-integral is 
\begin{align}
  \vect{K}_2 =& -\frac{\hbar c N}{2(2\pi)^{D/2}}\intzinf\frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}\oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}  \hspace{-4ex} dS
  \nonumber\\
  &\times\sum_{n=0}^{N-1}\sum_{m=1}^{N-n-1}
  \Bigdlangle(\vect{x}_0-\vect{R}_2)\wedge\hat{n}_2(\vect{x}_0) \nonumber\\
  &\hspace{2.5cm} \times\I[1]m \I[2]n  f_{n,m}\Bigdrangle_{\vect{x}(t)}.
\end{align}
and the potential curvature is given by
\begin{align}
  C_{ij} =& \frac{\hbar c N}{2(2\pi)^{D/2}}\intzinf\frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}\oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}  \hspace{-4ex} dS\, \hat{n}_1(\vect{x}_0)
  \nonumber\\ 
  &\times\biggdlangle 
  \sum_{k=0}^{N-1}\hat{n}_2(\vect{x}_k)\mathcal{G}(\vect{x}_0,\vect{x}_k,k,\cT)
  \nonumber\\
  &\hspace{0.75cm} \times\sum_{n=0}^{N-2}\sum_{m=0}^{N-n-2}\I[1]n\I[2]m g_{m,n}
  \biggdrangle_{\vect{x}(t)|\sigma_2(\vect{x}_k-\vect{R}_2)=0}
\end{align}
where 
\begin{align}
  g_{m,n}=c_{m+1,n+1}+c_{m,n}-c_{m+1,n}-c_{m,n+1},
\end{align}
accounts for the change in the energy integrand as the number of points in the first and second
bodies increase.
In the strong-coupling limit, the potential curvature is dominated by terms with $n=m=0$,
which correspond to paths that graze both bodies, while not entering either body.  

The formulation for the Casimir force in Eq.~(\ref{eq:occupation_force}) 
is exactly the construction of paths employed by Gies and Weber for computing 
forces in the sphere-plane and cylinder-plane geometries in the Dirichlet limit~\cite{Weber2010}.  
In that work paths are shifted so that the path just grazes the plane.  The force on the planar
surface is computed by integrating the over the times when the path intersects the spherical or cylindrical surface.
The expressions presented here extend their results by accounting for finite $\chi$, 
and are framed in terms of general geometries.  

In general, different classes of paths are important in the finite $\chi$ and strong-coupling 
cases.  At small $\chi$, the most important path statistic is the sojourn time within the bodies,
while in strong-coupling regime, the first-touching time is the most important statistic.    
This is correspondence was previously used to describe the numerical convergence properties of as 
the resolution of the paths was varied~\cite{Mackrory2016}.  
More practically, this makes it difficult to use a single class of loops to evaluate the potential at all $\chi$:
in weak-coupling one wants a path-ensemble that enters all of the bodies, while in strong-coupling
it is the paths that just touch the surfaces that are most important.

The expressions for the force in Eqs.~(\ref{eq:pinning_force}) and (\ref{eq:occupation_force})
reflect taking two limits in different orders, namely the taking the large $N$ limit and differentiation.  
The first derivation assumed an arbitrarily fine path where $N\gg \chi$ for all $\chi$.  Under
differentiation the arbitrarily fine paths can be pinned to the surface, and there is a range of 
$\cT$ where the integrand is non-zero.  However for a discrete path of length $N$, for sufficiently large $\chi$, 
this range of $\cT$ is inaccessible, and thus the naive numerical estimate fails.    
The second derivation instead takes the $N\rightarrow\infty$ expression last, while using well-behaved
expressions as $\chi\rightarrow\infty$, as is better suited to a numerical method based on discrete paths.  
This method instead highlights finding the times when the number of points inside each body 
change.  

We must distinguish between two facets of the different methods.
One is the choice of starting paths, and the form of the integrand.  In either case, path-pinning
or occupation, we are free to consider a single-path $\{B_j\}$ starting at $x_0$: $x_j=x_0+\sqrt{T}B_j$.
The is also an associated family of paths starting at $x_0$ that translate the original Brownian path
by $-\sqrt{T}B_k$: $x^k_j = x_0+\sqrt{T}(B_j-B_k)$.  This effectively changes which point on that bridge
corresponds to zero.  This sampling is essential for strong-coupling limits where only terms with no points
inside the body contribute to the force or potential curvature.  
This choice to average over which point on the path corresponds to zero is independent of the choice 
of the integrand.  




% It is convenient to decompose the Casimir energy as 
% \begin{equation}
%   E = \frac{\hbar c}{2(2\pi)^{D/2}}\int_0^\infty \frac{d\cT}{\cT^{1+D/2}} W,
% \end{equation}
% where the spatial path integral $W$ is defined as
% \begin{align}
%   W &:= \int d\vect{x}_0\biggdlangle \frac{1}{\langle\epsr\rangle^\alpha}-\frac{1}{[\epsr(\vect{x}_0)]^\alpha}\biggdrangle,
%   \label{eq:spatial_path_integral}
% \end{align}
% and $\alpha=1/2$.  
% We will focus our development on $W$, as this carries all of the essential spatial
% information, and the full path-integral expressions can be restored by including the $\cT$ integral and constants. 

% We consider a general geometry for computing Casimir forces between dielectrics, which is shown in Fig.~\ref{fig:spud_sketch}.
% The dielectric $\epsr(\vect{x})$ is specified by 
% \begin{equation}
%   \epsr(\vect{x}) = 1+\sum_j\chi_j\theta[\sigma_j(\vect{x}-\vect{R}_j)],
% \end{equation}
% where $\chi_j$ is the dielectric susceptibility of body $j$, $\sigma_j(\vect{x})=0$ 
% defines the surface of body $j$, with $\sigma_j>0$ and $\sigma_j<0$ inside and outside the body
% respectively, and $\vect{R}_j$ is the center of that body.  
% % \begin{figure}
% %   \includegraphics[width=\columnwidth]{fig/spud_sketch}
% %   \caption{Geometry for interacting dielectric bodies of susceptibility $\chi_j$, centered at
% %     $\vect{R}_j$ relative to the origin.  The surfaces mark $\sigma_j=0$, and the surface normal vectors $\hat{n}_j$
% %     are also marked.}
% %   \label{fig:spud_sketch}
% % \end{figure}

% The preceding methods offer an intuitive picture of the Casimir force,
% however they are poorly behaved as one takes the strong coupling limit.  
% For a typical path of $N$ steps pinned to the surface, approximately half 
% of the path will lie inside the body.  For $\chi\gg N$, the denominator $\langle\epsr\rangle^{-1/2}$ dominates
% the integrand, so estimate of the derivatives tends to zero as $\chi^{-1/2}$ for almost all paths.  
% Only rare paths which start on the surface, but do not enter the bulk of the body will contribute significantly.  
% As a result the estimated force goes to zero in the strong-coupling limit.
% In this section we develop an alternative expression which is better behaved in the strong-coupling
% limit and makes direct contact with prior work on Dirichlet worldlines.  


% This is exactly the construction of paths employed by Gies and Weber for computing 
% forces in the sphere-plate and cylinder-plate geometries in the Dirichlet limit~\cite{Weber2010}.  
% In that work they shift the paths so that the path just grazes the plane.  The force on the planar
% surface is computed by integrating the over the times when the path intersects the spherical or cylindrical surface.
% The expressions presented here extend their results by accounting for finite $\chi$, 
% and are framed in terms of general geometries.  

% In general, different classes of paths are important in the finite $\chi$ and strong-coupling 
% cases.  At small $\chi$, the most important path statistic is the sojourn time within the bodies,
% while in strong-coupling regime, the first-touching time is the most important statistic.    
% This is correspondence was previously used to describe the numerical convergence properties of as 
% the resolution of the paths was varied~\cite{Mackrory2016}.  
% More practically, this makes it difficult to use a single class of loops to evaluate the potential at all $\chi$:
% in weak-coupling one wants a path-ensemble that enters all of the bodies, while in strong-coupling
% it is the paths that just touch the surfaces that are most important.

% The expressions for the force in Eqs.~(\ref{eq:pinning_force}) and (\ref{eq:occupation_force})
% reflect taking two limits in different orders, namely the taking the large $N$ limit and differentiation.  
% The first derivation assumed an arbitrarily fine path where $N\gg \chi$ for all $\chi$.  Under
% differentiation the arbitrarily fine paths can be pinned to the surface, and there is a range of 
% $\cT$ where the integrand is non-zero.  However for a discrete path of length $N$, for sufficiently large $\chi$, 
% this range of $\cT$ is inaccessible, and thus the naive numerical estimate fails.    
% The second derivation instead takes the $N\rightarrow\infty$ expression last, while using well-behaved
% expressions as $\chi\rightarrow\infty$, as is better suited to a numerical method based on discrete paths.  
% This method instead highlights finding the times when the number of points inside each body 
% change.  

\subsection{Numerical Implementation of TE Casimir Forces}
\begin{align}
  \vect{F}_2 =& (-1)\frac{\hbar c N}{2(2\pi)^{D/2}}\intzinf \frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}\oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}  \hspace{-4ex} dS
\nonumber\\
  &\hspace{0.5cm}\times \sum_{n=0}^{N-1}\sum_{m=1}^{N-n-1}\bigdlangle\hat{n}_2(\vect{x}_0)
  \I[1]m\I[2]n f_{m,n}\bigdrangle_{\vect{x}(t)}
  \label{eq:occupation_force}
\end{align}
where the material dependence is carried by 
\begin{align}
  f_{m,n}&:=c_{m,n}-c_{m,n+1}-c_{0,n}+c_{0,n+1},\\
  c_{m,n} &:= \bigg( 1 + \frac{m\chi_1+n\chi_2}{N}\bigg)^{-\alpha},
\end{align}

\subsection{Numerical Implementation for TE Potential Curvature}

The path integral for the potential curvature in the occupation numbers apporach requires paths 
that are constrained by $\sigma_1(\vect{x}_j)$ and $\sigma_2(\vect{x}_k)$.  
There are a couple approaches to take here.  First, we could generate paths and then retroactively
find points close to the requisite surfaces, and perturb those paths such that the points do touch the 
surface.  We would then sum over all possible combinations of the indices close to the surfaces.
In this approach we would sample a position $x_0$, and then a time $\cT$.  Then we would find the 
near-intersections, and sum the integral over perturbing these.  

Recall, the potential curvature is 
\begin{align}
  C_{ij} =& \frac{\hbar c N}{2(2\pi)^{D/2}}\intzinf\frac{d\cT}{\cT^{1+D/2}}
  \hspace{-2ex}\oint\limits_{\sigma_2(\vect{x}_0-\vect{R}_2)=0}  \hspace{-4ex} dS\, \hat{n}_1(\vect{x}_0)
  \nonumber\\ 
  &\times\biggdlangle 
  \sum_{k=0}^{N-1}\hat{n}_2(\vect{x}_k)\mathcal{G}(\vect{x}_0,\vect{x}_k,k,\cT)
  \nonumber\\
  &\hspace{0.75cm} \times\sum_{n=0}^{N-2}\sum_{m=0}^{N-n-2}\I[1]n\I[2]m g_{m,n}
  \biggdrangle_{\vect{x}(t)|\sigma_2(\vect{x}_k-\vect{R}_2)=0}
\end{align}
where 
\begin{align}
  g_{m,n}=c_{m+1,n+1}+c_{m,n}-c_{m+1,n}-c_{m,n+1},
\end{align}

That effort scales quite badly roughly as an additional $N^2$.  
We could pick the a pair of fixed indices based on their probability of occuring?  
Secondly, we could also 

This integrand is non-zero for paths that touch both bodies.  In the strong-coupling limit,
only paths that just graze the bodies will contribute.  However at finite $\chi$, all paths 
are useful since paths with a finite sojourn time contribute significantly.  

This suggests a two-fold approach: generate paths constrained to touch without regard for their
occupation time (which will capture small $\chi$), and also isolate a subset of patsh which just touch the 
bodies (which will capture large $\chi$.  

We can either directly generate the paths that obey the constraints, or we can generate 
generic paths and then check whether these paths touch the desired surfaces.  

In the second strategy, the paths are generated, and we then check if there are any points
on the path which are close to a surface.  Mathematically 
$\text{min}_x|\vect{x}_j-\sigma_b(x)|\sim \sqrt{\Delta T}$.  We would then find all such points
for both bodies and then randomly select a pair to be pinned to the surface.
 We would also have to introduce the correction
for distorting this path point to lie on the surface,
\begin{equation}  
  P_{\text{pin-corr}} = \frac{P_{\text{pinned}}}{P_{\text{free}}}
  =\frac{ e^{-|\vect{x}_{j+1}-\vect{x}^*|^2/(2\Delta T)-|\vect{x}_{j-1}-\vect{x}^*|^2/(2\Delta T)  } }
  {e^{-|\vect{x}_{j+1}-\vect{x}_j|^2/(2\Delta T)-|\vect{x}_{j-1}-\vect{x}_j|^2/(2\Delta T)  }},
\end{equation}
where $x^*$ is the nearest point to the surface to $\vect{x}_j$.


\subsubsection{Direct Construction of constrained paths}

Alternatively,  we can enforce the touching constraint directly and make the paths touch the bodies by constructing
paths which are fixed at steps $j$ and $k$ to touch the surface, and sum over all $j,k\in {1,N}$.  
Evidently the direct approach will create many paths with small contributions, particular if $j-k \sim 1$.
This can be made more efficient by exploiting the Gaussian factor as a probability distribution.

In computing the energy the Gaussian is introduced for the purpose of importance sampling, and must
be factored out.  In this case, the Gaussian factor is already manifest, and should be taken into account
when sampling time.s   
We would sample times from Eq.~\ref{eq:expT}, with 
\begin{equation}
  T_0 = \frac{N^2d^2}{2\Delta(N-\Delta)} \label{eq:T0_curvature}
\end{equation}
where $\Delta$ is the difference in indices at the crossing points.
The result of using this is as our probability distribution is to factour out a normalization constant,
which leaves the integrand as 
\begin{equation}
  c=\frac{N}{\sqrt{2\pi\Delta(N-\Delta)}}\Gamma(s-1)\bigg(\frac{N^2d^2}{2\Delta(N-\Delta)}\bigg)^{-s+1}
    =\frac{2^{s-1}\Gamma(s-1)}{\sqrt{2\pi} d^{2s-2}}\bigg[\frac{\Delta(N-\Delta)}{N^2}\bigg]^{s-3/2}
\end{equation}
where the exponent $s=1+(D+1)/2=7/2$ accounts for the path integral normalization and the Gaussian factors of $T$ at
zero temperature.    
The values for the difference between the pinning indices $\Delta$ are found by numerical root-finding, using bisection.
The cumulative probability distribution for $\Delta$ is
\begin{gather}
 S_\Delta = \frac{1}{S_{N-1}}\sum_{j=1}^\Delta \bigg[\frac{j(N-j)}{N^2}\bigg]^{s-3/2}
\end{gather}
Given a uniform random number $u$, the corresponding index $\Delta$ must satisfy $S_{\Delta-1}<u<S_\Delta$.
The difference $\Delta$ can be found by bisection and starting with estimates at $S_0$ and $S_{N-1}$.
The final expression for the potential curvature is
\begin{align}
  C_{ij}%   =& \frac{\hbar c}{2(2\pi)^{D/2}}\sum_{j,\Delta>0}\int \frac{d\cT}{\cT^{1+D/2}}\biggdlangle \int d\vect{x}_0
%   \frac{N^2e^{-N^2d^2/[2 \Delta(N-\Delta)\cT]}}{\sqrt{2\pi \cT \Delta(N-\Delta)}}\nonumber\\
%   &\times
%    \hat{n}_1(\vect{x}_j)\hat{n}_2(\vect{x}_{k})
%    \sum_{n=0}^{N-2}\sum_{m=0}^{N-n-2}\I[1]n\I[2]m g_{m,n}
%    \biggdrangle_{\vect{x}(\cT);{\sigma_1(\vect{x}_j-\vect{R}_1)=0\atop \sigma_2(\vect{x}_{j+\Delta}-\vect{R}_2)=0}}\\
% =& \frac{\hbar c}{2(2\pi)^{D/2}}\sum_{j}\int d\vect{x}_0\biggdlangle 
% \frac{2^{s-1}\Gamma(s-1)S_{N-1}}{\sqrt{2\pi}|\vect{x}_j-\vect{x}_{j+\Delta}|^{2(s-1)}}
%   \hat{n}_1(\vect{x}_j)\hat{n}_2(\vect{x}_{k})
%   \sum_{n=0}^{N-2}\sum_{m=0}^{N-n-2}\I[1]n\I[2]m g_{m,n}
%   \biggdrangle_{\Delta,\cT,\vect{x}(\cT);{\sigma_1(\vect{x}_j-\vect{R}_1)=0\atop \sigma_2(\vect{x}_k-\vect{R}_2)=0}},
=& \frac{\hbar c}{2(2\pi)^{D/2}}\sum_{j}\int d\vect{x}_0\biggdlangle 
\frac{3S_{N-1}}{|\vect{x}_j-\vect{x}_{j+\Delta}|^{5}}
  \hat{n}_1(\vect{x}_j)\hat{n}_2(\vect{x}_{k})
  \sum_{n=0}^{N-2}\sum_{m=0}^{N-n-2}\I[1]n\I[2]m g_{m,n}
  \biggdrangle_{\Delta,\cT,\vect{x}(\cT);{\sigma_1(\vect{x}_j-\vect{R}_1)=0\atop \sigma_2(\vect{x}_k-\vect{R}_2)=0}},
\end{align}
where we have used $\dlangle \cdots\drangle_{y; C}$ to denote ensemble averaging over quantities 
$y$ subject to possible constraints $C$.  The first pinning positions $j$ are uniformly sampled over,
the second pinnings are sampled from $P_\Delta$ with $k=j+\Delta$, and the times are sampled from
$P_{\text{exp-T}}(T;T_0,1+(D+1)/2)$, with $T_0$ given by Eq.~(\ref{eq:T0_curvature}).  
The paths are still then constructed subject to the constraints of touching the bodies at the appropriate
indices.  The remaining integrand is evaluated for the resulting paths.  




\section{Numerical Results: Casimir-Polder Energies}

    \subsection{TE Polarization - Atom-Plane}
    \begin{enumerate}
      \item Simple Trapezoidal rule
      \item Convergence arguments
      \item ``Exact methods''
    \end{enumerate}

    \subsection{TE Polarization - Atom-Plane Gradients}

    
    \subsection{TM Polarization}

    \begin{enumerate}
      \item Need gradient estimation.  Use Malliavin calculus~\cite{Fournie1999, Chen2007,Kohatsu-Higa2003}.
        Formal introductions \cite{Nualart2006, Malliavin2006, DiNunno2009}.
      \item Birth-death methods to handle product of increments.  Related to Genealogical 
        methods used in rare-event simulation.  
      \item Histogram of raw values, with/without birth-death.
    \end{enumerate}


\section{Numerical Results: Casimir Energies}

\subsection{TE Casimir Energies - Plane-Plane}


\subsubsection{Scaling with N}


\subsection{TE Casimir Gradients - Plane-Plane}




\subsection{TM Casimir Energies - Plane-Plane}

\subsubsection{Scaling with N}


    
    % \section{Planar Dielectrics}




    % \section{Atom-Sphere}
    % \section{Atom-Cylinder}

    % \section{Plane-Sphere}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis_master"
%%% End: 
